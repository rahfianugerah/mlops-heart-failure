{
  "units1": 128,
  "units2": 16,
  "activation": "relu",
  "alpha": 0.005408287817976951,
  "learning_rate": 0.04099764994810526,
  "batch_size": 128,
  "epochs": 300
}